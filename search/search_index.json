{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-d2fengine","title":"Welcome to D2fEngine!","text":""},{"location":"#features","title":"Features","text":""},{"location":"#future-works","title":"Future Works","text":""},{"location":"license/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 DENG Lab @ SJTU</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"impl/0_overview/","title":"Overview","text":""},{"location":"impl/0_overview/#nano-vllm-to-d2fengine","title":"Nano-vLLM to D2fEngine","text":""},{"location":"impl/0_overview/#highlights","title":"Highlights","text":""},{"location":"impl/0_overview/#0x00-basic-vllm-features","title":"0x00. Basic vLLM Features","text":"<ul> <li>Paged Attention for dLLM: Supports PagedAttention for Diffusion LLM to handle long sequences efficiently.</li> <li>Dynamic Batching: Supports dynamic batching with flexible sequence lengths.</li> <li>Tensor Parallelism + Data Parallelism: Supports tensor parallelism and data parallelism for efficient inference.</li> </ul>"},{"location":"impl/0_overview/#0x01-fully-support-for-d2f-inference","title":"0x01. Fully Support for D2F Inference","text":"<p>D2fEngine fully supports D2F inference: piplined block parallel decoding. Also, with proper configuration, it can support Block Diffusion inference without any training.</p>"},{"location":"impl/0_overview/#0x02-distinct-kv-cache-layout-more-efficient-developing","title":"0x02. Distinct KV Cache Layout (more efficient, developing)","text":"<p>1. Fused Varlen Parallel Decoding Kernel</p> <p>The core algorithm the kernel comes from FlashAttention2. </p> <p>Major points:</p> <ul> <li> <p>Shapes of inputs:</p> <ul> <li><code>q</code>: [TotalSeqLen, NumHeads, HeadDim]</li> <li><code>k</code>, <code>v</code>: [TotalSeqLen, NumKvHeads, HeadDim]</li> <li><code>kcache</code>: [NumPages, NumKvHeads, HeadDim // x, PageSize, x]</li> <li><code>vcache</code>: [NumPages, NumKvHeads, HeadDim, PageSize]</li> <li><code>mask</code>: [TotalSeqLen, TotalSeqLen] (block diagonal mask, inner mask is block-wise causal)</li> </ul> </li> <li> <p>Other inputs:</p> <ul> <li><code>page/block_table</code>: [NumSeqs, MaxNumPages]</li> <li><code>cu_seqlens_q</code>: [NumSeqs + 1]</li> <li><code>ctxlens</code>: [NumSeqs]</li> <li><code>total_seqlens</code>: [NumSeqs]</li> <li><code>slot_mapping</code>: [TotalSeqLen]</li> </ul> </li> <li> <p>Three constexpr parameters: <code>BLOCK_M</code> (for <code>q</code> tiles), <code>BLOCK_N</code> (for <code>kv</code> tiles), and <code>PAGE_SIZE</code> (for <code>kvcache</code> tiles).</p> </li> <li> <p>Three accumulation registers: <code>acc</code> (outputs, [BLOCK_M, BLOCK_N]), <code>m_i</code> (rowise max elems for online-softmax, [BLOCK_M, ]), and <code>l_i</code> (rowise factors of online-softmax, [BLOCK_M, ]).</p> </li> </ul> <p>Kernel Pipeline:</p> <ul> <li> <p>[STAGE 1] Attention Against KV Cache: Load <code>kvcache</code> and compute attention of <code>q</code> &amp; <code>kvcache</code> with fixed block size (tile size) equals to the <code>PAGE_SIZE</code> of <code>page/block_table</code>. Registering all the result of current block of q in <code>acc</code>, <code>m_i</code>, <code>l_i</code>.</p> </li> <li> <p>[STAGE 2] Input QKV Self-Attention: Load <code>kv</code> and compute attention of <code>q</code> &amp; <code>kv</code> with fixed block size (tile size) equals to <code>BLOCK_N</code>, along with custom <code>mask</code> applying. Registering all the result of current block of q in <code>acc</code>, <code>m_i</code>, <code>l_i</code>. </p> </li> <li> <p>[STAGE 3] Output: Store the result of <code>acc</code> to <code>out</code>.</p> </li> </ul> <p>2. Parallel KV Cache Store Kernel (Distinct)</p> <p>Parallel KV Cache Store Kernel (Distinct) is designed to store the KV cache in a distinct layout, which is more efficient for D2fEngine. </p> <p>What should be noted is that because the input <code>kv</code> is not all being stored every time call the kernel, thus the <code>slot_mapping</code> preparation process is way different that of traditional vLLM designed specifically for Auto-Regressive LLMs.</p>"},{"location":"impl/0_overview/#0x03-unified-kv-cache-layout","title":"0x03. Unified KV Cache Layout","text":"<p>1. Parallel KV Cache Loading/Storing Kernels + FlexAttention Implemented Block Diagonal Attention</p> <p>Major points:</p> <p>Except for the <code>kvcache</code> layout, the other inputs are the same to the Fused Varlen Parallel Decoding Kernel.</p> <ul> <li><code>kvcache</code>: [NumPages, PageSize, NumKvHeads, HeadDim] (same as the flash kv cache of vLLM, used for flash attention)</li> </ul> <p>Function Pipeline:</p> <p>The Parallel KV Cache Storing (Unified) is similar to the Parallel KV Cache Storing (Distinct) except for the fetching indices build-up procedure difference caused by the different <code>kvcache</code> layout.</p> <p>Generate in-sequence-masks from <code>Sequence</code> , which is a block-wise causal mask, then in the <code>Context</code> post-init, generate the block-diagonal mask for the whole batched input sequence.</p> <p>Using the block-diagonal mask, the FlexAttention Implemented Block Diagonal Attention can be used to compute the attention of <code>q</code> &amp; <code>kvcache</code> with block sparsity.</p>"},{"location":"impl/0_overview/#demo-benchmark","title":"Demo Benchmark","text":""},{"location":"impl/0_overview/#configuration","title":"Configuration","text":"Parameter Value enforce_eager True data_parallel_size 8 tensor_parallel_size 1 gpu_memory_utilization 0.3 max_num_batched_tokens 5120 max_num_seqs 20 max_model_len 5120 <p>Devices: 8x Nvidia H20-NVLink 96G </p>"},{"location":"impl/0_overview/#dream-v0-base-7b","title":"Dream-v0-Base-7B","text":""},{"location":"impl/0_overview/#gsm8k-cot","title":"GSM8K-CoT","text":"Metric Value Total Samples 1319 Total Tokens 336429 Metric D2fEngine D2F-Baseline Base Model Total Time 380.79 (92.8x) 3693.20 (9.6x) 35349.20 (1.0x) TPS 883.51 (93.0x) 91.2 (9.6x) 9.5 (1.0x) AVG Latency 0.29 (92.4x) 2.8 (9.6x) 26.8 (1.0x)"},{"location":"impl/0_overview/#humaneval","title":"HumanEval","text":"Metric Value Total Samples 164 Total Tokens 36198 Metric D2fEngine D2F-Baseline Base Model Total Time 49.27 (41.9x) 508.40 (4.1x) 2066.4 (1.0x) TPS 734.68 (36.4x) 73.2 (3.6x) 20.2 (1.0x) AVG Latency 0.30 (42.0x) 3.1 (4.1x) 12.6 (1.0x)"}]}